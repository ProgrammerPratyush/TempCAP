import streamlit as st
import openai
from langchain_community.chat_models import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, PromptTemplate
from langchain.schema import Document
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
import torch.nn as nn
import os
import shutil
import torch
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import numpy as np
from typing import List

# my imports -
from torchvision import transforms
from PIL import Image
from io import BytesIO
from streamlit_geolocation import streamlit_geolocation
import requests
import pandas as pd
from datetime import datetime, timedelta
import sqlite3
from tensorflow.keras.layers import GlobalAveragePooling2D, BatchNormalization, Dense, Dropout
from tensorflow.keras.applications import ResNet50

# Load environment variables
load_dotenv()

# Set OpenAI API key from environment variable
openai.api_key = os.environ['OPENAI_API_KEY']

CHROMA_PATH = "chroma"
DATA_PATH = "data/"

# Load your pre-trained models
# plant_disease_model_weights_path = 'models for predictions/Crops/plant-disease-model.pth'
plant_disease_model_path = 'models for predictions/Crops/plant-disease-model-complete.pth'
soil_composition_model_path = 'models for predictions/Soil/composition/composition/two.h5'
soil_type_model_weights_path = 'models for predictions/Soil/soil type/soiltype/tycoon.weights.h5'

GENERATIVE_PROMPT_TEMPLATE = """
Generate a response to the following question, even if no direct context is provided:

Question: {question}
"""

PROMPT_TEMPLATE = """
Answer the question based only on the following context:
{context}

Question: {question}
"""

# Function to create or update the Chroma vector store
def generate_data_store():
    documents = load_documents()
    chunks = split_text(documents)
    save_to_chroma(chunks)

def load_documents():
    loader = DirectoryLoader(DATA_PATH, glob="*.txt", show_progress=True)
    documents = loader.load()
    return documents

def split_text(documents: List[Document]):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,
        chunk_overlap=100,
        length_function=len,
        add_start_index=True,
    )
    chunks = text_splitter.split_documents(documents)
    return chunks

def save_to_chroma(chunks: List[Document]):
    # Clear out the database first
    if os.path.exists(CHROMA_PATH):
        shutil.rmtree(CHROMA_PATH)

    # Use OpenAI embeddings
    embedding_function = OpenAIEmbeddings(openai_api_key=openai.api_key)

    # Create a new DB from the documents
    db = Chroma.from_documents(
        chunks, embedding_function, persist_directory=CHROMA_PATH
    )
    db.persist()

def reciprocal_rank_fusion(results: List[List], k=60):
    fused_scores = {}
    for docs in results:
        for rank, doc in enumerate(docs):
            doc_str = doc.json()
            if doc_str not in fused_scores:
                fused_scores[doc_str] = 0
            fused_scores[doc_str] += 1 / (rank + k)

    reranked_results = sorted(
        [(doc, score) for doc, score in fused_scores.items()],
        key=lambda x: x[1],
        reverse=True
    )
    return [doc for doc, _ in reranked_results]

# Function to process the query and generate a response using RAG Fusion
def process_query(query_text):
    embedding_function = OpenAIEmbeddings(openai_api_key=openai.api_key)
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)

    # Perform similarity search to get relevant documents and their scores
    results = db.similarity_search_with_relevance_scores(query_text, k=5)

    if len(results) == 0 or results[0][1] < 0.7:
        # If no relevant context is found or relevance score is below threshold, use GPT for response
        source = "Intelligently generated by our AI model"
        context_text = "N/A"
        generative_prompt_template = ChatPromptTemplate.from_template(GENERATIVE_PROMPT_TEMPLATE)
        generative_prompt = generative_prompt_template.format(question=query_text)
        model = ChatOpenAI(api_key=openai.api_key, model="gpt-4o")
        response = model.predict(generative_prompt)
        response_text = response.strip()

    else:
        # If relevant context is found, use RAG Fusion Chain
        source = "Based upon the papers and publications by authorized bodies."
        context_text = "\n\n---\n\n".join([doc.page_content for doc, _score in results])
        prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
        prompt = prompt_template.format(context=context_text, question=query_text)

        # Set up RAG Fusion Chain
        prompt_chain = ChatPromptTemplate(
            input_variables=['question'],
            messages=[
                SystemMessagePromptTemplate(
                    prompt=PromptTemplate(
                        input_variables=[],
                        template='You are a helpful assistant that generates multiple search queries based on a single input query.'
                    )
                ),
                HumanMessagePromptTemplate(
                    prompt=PromptTemplate(
                        input_variables=['question'],
                        template='Generate multiple search queries related to: {question} \n OUTPUT (4 queries):'
                    )
                )
            ]
        )

        generate_queries = (
            prompt_chain | ChatOpenAI(api_key=openai.api_key, model="gpt-4o") | StrOutputParser() | (lambda x: x.split("\n"))
        )

        ragfusion_chain = generate_queries | db.as_retriever().map() | reciprocal_rank_fusion

        full_rag_fusion_chain = (
            {
                "context": ragfusion_chain,
                "question": RunnablePassthrough()
            }
            | ChatPromptTemplate.from_template("""Answer the question based only on the following context:
            {context}

            Question: {question}
            """)
            | ChatOpenAI(api_key=openai.api_key, model="gpt-4o")
            | StrOutputParser()
        )

        # Get the response from the RAG Fusion chain
        response = full_rag_fusion_chain.invoke({"question": query_text})
        response_text = response.strip()

    # Formatting the response
    formatted_response = f"""
    1. **Actual query by user**: {query_text}

    2. **Source**: {source}

    3. **Name of the crop**: 

    4. **Response from the RAG model**: 
    {response_text}
    """

    return formatted_response.strip()


# Crop Disease Prediction Function
class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, pool=False):
        super(ConvBlock, self).__init__()
        layers = [
            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding),
            nn.ReLU(),
            nn.BatchNorm2d(out_channels)
        ]
        if pool:
            layers.append(nn.MaxPool2d(2))
        self.block = nn.Sequential(*layers)

    def forward(self, x):
        return self.block(x)

class ResNet9(nn.Module):
    def __init__(self, in_channels, num_diseases):
        super().__init__()

        self.conv1 = ConvBlock(in_channels, 64)
        self.conv2 = ConvBlock(64, 128, pool=True)
        self.res1 = nn.Sequential(ConvBlock(128, 128), ConvBlock(128, 128))

        self.conv3 = ConvBlock(128, 256, pool=True)
        self.conv4 = ConvBlock(256, 512, pool=True)
        self.res2 = nn.Sequential(ConvBlock(512, 512), ConvBlock(512, 512))

        self.classifier = nn.Sequential(nn.MaxPool2d(4),
                                       nn.Flatten(),
                                       nn.Linear(512, num_diseases))

    def forward(self, xb):
        out = self.conv1(xb)
        out = self.conv2(out)
        out = self.res1(out) + out
        out = self.conv3(out)
        out = self.conv4(out)
        out = self.res2(out) + out
        out = self.classifier(out)
        return out

# Function to preprocess the image
def preprocess_image(image):
    transform = transforms.Compose([
        transforms.Resize((256, 256)),  # Increase the input size
        transforms.ToTensor(),
    ])
    img = transform(image).unsqueeze(0)  # Add batch dimension
    return img


# Prediction function using the fully saved PyTorch model
def predict_crop_disease(image, model_path):
    try:
        # Load the complete model directly
        model = torch.load(model_path, map_location=torch.device('cpu'))
        model.eval()

        # Preprocess the image
        img = preprocess_image(image)

        # Make predictions
        with torch.no_grad():
            predictions = model(img)
        predicted_class = torch.argmax(predictions, dim=1).item()

        # Map the prediction to class name
        return class_names[predicted_class]
    except Exception as e:
        print(f"Error in predicting crop disease: {e}")
        return None


# List of disease names corresponding to class indices
class_names = [
    "Apple___Apple_scab",
    "Apple___Black_rot",
    "Apple___Cedar_apple_rust",
    "Apple___healthy",
    "Blueberry___healthy",
    "Cherry_(including_sour)___Powdery_mildew",
    "Cherry_(including_sour)___healthy",
    "Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot",
    "Corn_(maize)___Common_rust_",
    "Corn_(maize)___Northern_Leaf_Blight",
    "Corn_(maize)___healthy",
    "Grape___Black_rot",
    "Grape___Esca_(Black_Measles)",
    "Grape___Leaf_blight_(Isariopsis_Leaf_Spot)",
    "Grape___healthy",
    "Orange___Haunglongbing_(Citrus_greening)",
    "Peach___Bacterial_spot",
    "Peach___healthy",
    "Pepper,_bell___Bacterial_spot",
    "Pepper,_bell___healthy",
    "Potato___Early_blight",
    "Potato___Late_blight",
    "Potato___healthy",
    "Raspberry___healthy",
    "Soybean___healthy",
    "Squash___Powdery_mildew",
    "Strawberry___Leaf_scorch",
    "Strawberry___healthy",
    "Tomato___Bacterial_spot",
    "Tomato___Early_blight",
    "Tomato___Late_blight",
    "Tomato___Leaf_Mold",
    "Tomato___Septoria_leaf_spot",
    "Tomato___Spider_mites Two-spotted_spider_mite",
    "Tomato___Target_Spot",
    "Tomato___Tomato_Yellow_Leaf_Curl_Virus",
    "Tomato___Tomato_mosaic_virus",
    "Tomato___healthy"
]

# District soil type code ---------------------------------------------------------------------
# Connect to the SQLite database
def load_data():
    conn = sqlite3.connect('soiltype.db')
    query = "SELECT DISTINCT `State Name`, `District` FROM soiltype;"
    df = pd.read_sql(query, conn)
    conn.close()
    return df

def get_soil_type(state, district):
    conn = sqlite3.connect('soiltype.db')
    query = """
    SELECT `Soil Type`
    FROM soiltype
    WHERE `State Name` = ? AND `District` = ?
    """
    df = pd.read_sql(query, conn, params=(state, district))
    conn.close()
    if not df.empty:
        return df.iloc[0]['Soil Type']
    else:
        return "Soil Type not found"

# Load data from the database
data = load_data()
# --------------------------------------------------------------------------------------------------------

# Soil Composition Prediction Function------------------------------------------------------------------------
def predict_soil_composition(img):
    img = Image.open(img)
    img = img.resize((224, 224))  # Resize to the expected input size
    img = np.array(img) / 255.0  # Normalize
    img = np.expand_dims(img, axis=0)  # Add batch dimension
    model = load_model(soil_composition_model_path)
    predictions = model.predict(img)
    return predictions[0]


# Define image dimensions
img_width, img_height = 224, 224

# Manually specify the class labels
class_labels = {0: 'Black Soil', 1: 'Cinder Soil', 2: 'Red Soil', 3: 'Yellow Soil'}

def load_and_preprocess_image(img):
    img = img.resize((img_width, img_height))
    img_array = np.array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = tf.keras.applications.resnet50.preprocess_input(img_array)
    return img_array

# Recreate the model architecture exactly as in training
base_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=(img_width, img_height, 3))

# Fine-tuning: Unfreeze the last 10 layers
for layer in base_model.layers[:-10]:
    layer.trainable = False

# Add custom classifier layers with dropout and batch normalization for regularization
model = tf.keras.models.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(4, activation='softmax')
])

# Compile the model with the same configuration as training
initial_learning_rate = 1e-4
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])

# Load the weights
model.load_weights('models for predictions/Soil/soil type/soiltype/tycoon.weights.h5')

# Weather API Function----------------------------------------------------------------------------------
# Function to get weather data from Visual Crossing API
def get_weather_data(lat, lon, start_date, end_date, api_key):
    url = f"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{lat},{lon}/{start_date}/{end_date}"

    params = {
        'unitGroup': 'metric',  # Use 'us' for Fahrenheit and 'metric' for Celsius
        'key': api_key,
        'include': 'days',  # Include daily data
        'elements': 'tempmax,tempmin,precip,windspeed'
    }

    response = requests.get(url, params=params)
    if response.status_code == 200:
        data = response.json()
        return data['days']
    else:
        st.error(f"Error: {response.status_code}")
        return None


# Function to create a summary string from the weather data
def create_summary(weather_data, period_name):
    if not weather_data:
        return f"No data available for the {period_name}."

    temps_max = [day['tempmax'] for day in weather_data if 'tempmax' in day]
    temps_min = [day['tempmin'] for day in weather_data if 'tempmin' in day]
    precip = [day.get('precip', 0) for day in weather_data]
    wind_speeds = [day['windspeed'] for day in weather_data if 'windspeed' in day]

    summary = (
        f"Weather Summary for {period_name} is that, "
        f"Max Temp: {max(temps_max, default='N/A')}°C, Min Temp: {min(temps_min, default='N/A')}°C, "
        f"Total Precipitation: {sum(precip)} mm\n and "
        f"Average Wind Speed: {sum(wind_speeds) / len(wind_speeds) if wind_speeds else 'N/A'} km/h\n"
    )
    return summary


# API Key
api_key = 'WYJXSZL8MMXELRQL9U3R2P6WR'  # Replace with your Visual Crossing API key

# ---------------------------------------------------------------------------------------------------------

# Prediction variables
# Initialize session state variables to store predictions and weather summary
if 'predicted_crop_disease' not in st.session_state:
    st.session_state.predicted_crop_disease = None
if 'predicted_soil_composition' not in st.session_state:
    st.session_state.predicted_soil_composition = None
if 'predicted_soil_type' not in st.session_state:
    st.session_state.predicted_soil_type = None
if 'weather_summary' not in st.session_state:
    st.session_state.weather_summary = None
if 'weather_button_clicked' not in st.session_state:
    st.session_state.weather_button_clicked = False
if 'formatted_response' not in st.session_state:
    st.session_state.formatted_response = ""

# Streamlit UI Layout
st.set_page_config(page_title="AgriVision: farmer's MITRA", page_icon="🌾")

st.title("AgriVision: Your Agricultural Assistant")

# Crop Disease Prediction Section
st.header("Predict Crop Disease")
crop_image = st.file_uploader("Upload a Crop Image", type=["jpg", "png", "jpeg", "JPG"])

if crop_image:
    st.image(crop_image, caption='Uploaded Crop Image', use_column_width=True)

    if st.button("Predict Disease"):
        image = Image.open(crop_image).convert('RGB')
        st.session_state.predicted_crop_disease = predict_crop_disease(image, plant_disease_model_path)

        if st.session_state.predicted_crop_disease is not None:
            st.success(f"Predicted Crop Disease: {st.session_state.predicted_crop_disease}")
        else:
            st.error("Error in predicting crop disease. Please check the logs.")

# Soil Composition Prediction Section
st.header("Predict Soil Composition")
soil_image_composition = st.file_uploader("Upload a Soil Image for Composition", type=["jpg", "png", "jpeg", "JPG"], key="composition")

if soil_image_composition is not None:
    st.image(soil_image_composition, caption='Uploaded Soil Image', use_column_width=True)

    if st.button("Predict Composition"):
        st.session_state.predicted_soil_composition = predict_soil_composition(soil_image_composition)
        st.write(f'The predicted soil composition is: {st.session_state.predicted_soil_composition}')

# Soil Type Prediction Section
st.header("Predict Soil Type")
soil_image_type = st.file_uploader("Upload a Soil Image for Type", type=["jpg", "png", "jpeg", "JPG"], key="type")

if soil_image_type is not None:
    st.image(soil_image_type, caption='Uploaded Soil Image', use_column_width=True)

    if st.button("Predict Soil Type"):
        image = Image.open(BytesIO(soil_image_type.read()))
        input_image = load_and_preprocess_image(image)
        predictions = model.predict(input_image)
        predicted_class = np.argmax(predictions, axis=1)
        st.session_state.predicted_soil_type = class_labels[predicted_class[0]]
        # Display the prediction result
        st.write(f'The predicted soil type is: {st.session_state.predicted_soil_type}')

# State wise soil type code -
st.header('State-vise Soil Type⛰️')

# Dropdowns for State Name and District
state = st.selectbox('Select State Name', data['State Name'].unique())
district = st.selectbox('Select District', data[data['State Name'] == state]['District'].unique())

# Display the Soil Type based on selections
if state and district:
    soil_type = get_soil_type(state, district)
    st.write(f"**Soil Type:** {soil_type}")

# Create a session state variable to track if the button has been pressed---------------------------------------
# Weather Prediction Section
st.header("Get My Location Weather🌦️")

# Weather Prediction Button
if st.button("Predict Weather"):
    st.session_state.weather_button_clicked = True

# Only run the weather prediction if the button has been clicked
if st.session_state.weather_button_clicked:
    location = streamlit_geolocation()

    if location and location['latitude'] and location['longitude']:
        latitude = location['latitude']
        longitude = location['longitude']

        # Dates
        end_date = datetime.now().strftime('%Y-%m-%d')
        start_date_last_month = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')  # Last 1 month
        start_date_next_15_days = datetime.now().strftime('%Y-%m-%d')
        end_date_next_15_days = (datetime.now() + timedelta(days=15)).strftime('%Y-%m-%d')

        # Get historical weather data for the last 1 month
        historical_weather = get_weather_data(latitude, longitude, start_date_last_month, end_date, api_key)

        # Get weather forecast for the next 15 days
        forecast_weather = get_weather_data(latitude, longitude, start_date_next_15_days, end_date_next_15_days, api_key)

        # Create summaries
        historical_summary = create_summary(historical_weather, "Last 1 Month")
        forecast_summary = create_summary(forecast_weather, "Next 15 Days")

        st.session_state.weather_summary = f"Historical Weather: {historical_summary}\nForecast Weather: {forecast_summary}"

        # Display summaries in Streamlit text boxes
        st.subheader("Historical Weather Summary")
        st.text(historical_summary)

        st.subheader("Forecast Weather Summary")
        st.text(forecast_summary)
    else:
        st.error("Location data is not available.")


# Function to print the generated report

def print_report(response_text):
    st.download_button(
        label="Download Report",
        data=response_text,
        file_name="report.txt",
        mime="text/plain",
    )

# res= ""

# Submit and Generate Report
if st.button("Submit"):
    crop_disease = st.session_state.predicted_crop_disease if st.session_state.predicted_crop_disease is not None else 'No prediction made'
    soil_composition = st.session_state.predicted_soil_composition if st.session_state.predicted_soil_composition is not None else 'No prediction made'
    soil_type = st.session_state.predicted_soil_type if st.session_state.predicted_soil_type is not None else 'No prediction made'
    weather_summary = st.session_state.weather_summary if st.session_state.weather_summary is not None else 'No weather data available'

    if crop_disease or soil_composition or soil_type or weather_summary:
        st.session_state.formatted_response = f"""
        1. **Predicted Crop Disease**: {crop_disease}

        2. **Predicted Soil Composition**: {soil_composition}

        3. **Predicted Soil Type**: {soil_type}

        4. **Weather Information**: {weather_summary}
        """

        st.subheader("Generated Report")
        st.write(st.session_state.formatted_response)

    else:
        st.warning("No predictions were made. Please ensure all sections have been filled.")


def print_pred_report(res):
    st.download_button(
        label="Download Prediction Report",
        data=res,
        file_name="prediction_report.txt",
        mime="text/plain",
    )

if st.button("Get Prediction Report"):
    if st.session_state.formatted_response:
        st.subheader("Print Prediction Report")
        print_pred_report(st.session_state.formatted_response)


# RAG Farmers Assistant Chat Section
st.header("Ask the RAG Farmers Assistant")
query = st.text_input("Enter your question here:")
if query:
    response = process_query(query)
    st.write(response)

if st.button("Get RAG-AI Assistance"):
    if query:
        response_text = process_query(query)
        st.subheader("Generated Response")
        st.write(response_text)

        st.subheader("Print Report")
        print_report(response_text)
    else:
        st.warning("Please enter a query before submitting.")

# Option to update the Chroma vector store
if st.button("Update Database"):
    generate_data_store()
    st.success("Database updated successfully.")